{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Notebook to Use Decision Tree Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to train a decision tree to classify unseen instances.\n",
    "\n",
    "For those of you interested in understanding the code, it uses predefined functions from the [sklearn](http://scikit-learn.org) library of machine learning primitives and from the [graphviz](http://www.graphviz.org) library to generate visualizations. A few more details about the code:  \n",
    "* The variable \"dataset\" stores the name of text file that you input and is passed as an argument of the function \"loadDataSet()\".  \n",
    "* The variable \"attributes\" stores the names of all features. The variable \"instances\" stores the values of all features in the training set. The variable \"labels\" stores the labels of all instances.  \n",
    "* The variable \"clf\" stores a decision tree model, and it can be trained with \"instances\" and \"labels\". Once the model is trained, it can be used to predict unseen instances.  We use a type of decision tree algorithm called CART (Classification and Regression Trees). \n",
    "* The variable \"n_foldCV\" stores the number of times of n-fold cross validation that you input.\n",
    "* The function \"cross_val_scores\" assesses the accuracy scores of a decision tree model.  Its inputs are \"clf\", \"instances\", \"labels\", \"n_foldCV\".\n",
    "* The variable \"scores\" stores the accuracy of an n-fold cross validation of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import graphviz\n",
    "\n",
    "def loadDataSet(dataset):\n",
    "    with open(dataset) as f:\n",
    "        data=f.readlines()\n",
    "        attributes=data[0].rstrip().split(',')[:-1]\n",
    "        instances=[entry.rstrip().split(',')[:-1] for entry in data[1:]]\n",
    "        dataArray=[]\n",
    "        Dict={}\n",
    "        for i in range(len(instances[0])):\n",
    "            try:\n",
    "                dataArray.append([float(instance[i]) for instance in instances])\n",
    "            except:\n",
    "                encodedData,vocab=encode([instance[i] for instance in instances])\n",
    "                dataArray.append(encodedData)\n",
    "                Dict[i]=vocab\n",
    "                print(attributes[i],': ',list(vocab.items()))\n",
    "        instances=np.array(dataArray).T\n",
    "        labels=[entry.rstrip().split(',')[-1] for entry in data[1:]]\n",
    "        return attributes,instances,labels,Dict\n",
    "\n",
    "def encode(data):\n",
    "    vocab={}\n",
    "    uniqueVals=list(set(data))\n",
    "    for Val in uniqueVals:\n",
    "        vocab[Val]=uniqueVals.index(Val)\n",
    "    encodedData=list(map(uniqueVals.index,data))\n",
    "    return encodedData,vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Building a Decision Tree Classifier ##\n",
    "\n",
    "The cell below asks for a dataset. It trains a decision tree classifier. \n",
    "\n",
    "We provide two classification datasets that could be applied to the decision tree algorithms. \n",
    "* [\"iris.txt\"](https://archive.ics.uci.edu/ml/datasets/iris) has four attributes with continuous values describing three different iris species.\n",
    "* [\"lenses.txt\"](https://archive.ics.uci.edu/ml/datasets/lenses) contains four attributes with discrete values and three classes.\n",
    "\n",
    "Before training your classifier, run the cell below to take a look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset=input('Please Enter Your Dataset:')\n",
    "df=pd.read_csv(dataset)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run the following cell, let's learn an important concept called feature encoding. Many classifiers only take numerical data and some datasets have features that are not numerical. For example, a feature can be the state that a person lives in. Those are called [categorical features](https://en.wikipedia.org/wiki/Categorical_variable). In that case,we need to encode categorical features into discrete values. This process is called feature encoding\n",
    "\n",
    "In our notebook, if your dataset contains categorical features, you will see the code rules in the cell below. In the next section, when you are prompted to input test set for prediction, the algorithm will automatically encode the relevant categorical features according to the code rules showned below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes,instances,labels,Dict=loadDataSet(dataset)\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(instances,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing a Decision Tree##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will generate a visualization of the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None,max_depth=5,\\\n",
    "feature_names=attributes,class_names=clf.classes_,label='all',\\\n",
    "filled=True,special_characters=True) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction: Classifying New Instances Using a Decision Tree Classifier##\n",
    "\n",
    "The cell below classifies new instances with the decision tree you created.\n",
    "\n",
    "When you are prompted to input a test set, please create an example of an instance that looks like the instances in the training set.  For example, if you trained the classifier with contact lenses data, you should create an instance that has the same kinds of features.  For example:\n",
    "\n",
    "\"young,myope,yes,normal\"\n",
    "\n",
    "\n",
    "Each feature value is separated with a comma, and should have the same length as the instances in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset=input('Please Enter Your Test Set:')\n",
    "testset=testset.strip().split(\",\")\n",
    "temp=[]\n",
    "for i in range(len(testset)):\n",
    "    try:\n",
    "        temp.append(float(testset[i]))\n",
    "    except:\n",
    "        temp.append(Dict[i][testset[i]])\n",
    "testset=np.array(temp).reshape((1,len(temp)))\n",
    "predictions=clf.predict(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Accuracy of a Decision Tree Classifier##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will run cross-validation to evaluate your decision tree classifier.  It will ask you for your test data, and the number of folds that you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=input('Please Enter Your Test Data:')\n",
    "n_foldCV=int(input(\"Please Enter the Number of Folds:\"))\n",
    "attributes,instances,labels,Dict=loadDataSet(dataset)\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(instances,labels)\n",
    "scores = cross_val_score(clf, instances, labels, cv=n_foldCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will output the accuracy score for each fold and the accuracy estimate of the model under 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sores:\")\n",
    "[print(score) for score in scores]\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What is the overall accuracy of the classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now you can print this notebook as a PDF file and turn it in. Note: The decision tree may be truncated during export. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
