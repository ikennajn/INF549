{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook1_Decision_Tree_Classifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HlO9vzSQGVB2"
   },
   "source": [
    "This notebook shows how to train a decision tree to classify unseen instances.\n",
    "\n",
    "For those of you interested in understanding the code, it uses predefined functions from the [sklearn](http://scikit-learn.org) library of machine learning primitives and from the [graphviz](http://www.graphviz.org) library to generate visualizations. A few more details about the code:  \n",
    "* The variable \"dataset\" stores the name of text file that you input and is passed as an argument of the function \"loadDataSet()\".  \n",
    "* The variable \"attributes\" stores the names of all features. The variable \"instances\" stores the values of all features in the training set. The variable \"labels\" stores the labels of all instances.  \n",
    "* The variable \"clf\" stores a decision tree model, and it can be trained with \"instances\" and \"labels\". Once the model is trained, it can be used to predict unseen instances.  We use a type of decision tree algorithm called CART (Classification and Regression Trees). \n",
    "* The variable \"n_foldCV\" stores the number of times of n-fold cross validation that you input.\n",
    "* The function \"cross_val_scores\" assesses the accuracy scores of a decision tree model.  Its inputs are \"clf\", \"instances\", \"labels\", \"n_foldCV\".\n",
    "* The variable \"scores\" stores the accuracy of an n-fold cross validation of the model.\n",
    "\n",
    "The first step is to get the data. the wget package allows to download datasets directly from the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rXR6i7obZO7v"
   },
   "outputs": [],
   "source": [
    "pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L_HVmfVTGWVk"
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "wget.download('https://raw.githubusercontent.com/khider/INF549/master/Assignment3_MachineLearning/Dataset/iris.csv')\n",
    "wget.download('https://raw.githubusercontent.com/khider/INF549/master/Assignment3_MachineLearning/Dataset/lenses.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines personalized functions to be used throughout the notebook. Make sure you execute this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J_aMOusiGeof"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "cmap_data = plt.cm.Paired\n",
    "cmap_cv = plt.cm.coolwarm\n",
    "\n",
    "def plot_cv_indices(cv, X, y, ax, n_splits, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "\n",
    "    # Plot the data classes and groups at the end\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "              c=y, marker='_', lw=lw, cmap=cmap_data)\n",
    "    \n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + ['class']\n",
    "    ax.set(yticks=np.arange(n_splits+1) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits+2.2, -.2], xlim=[0, len(X)])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax\n",
    "\n",
    "def loadDataSet(dataset):\n",
    "    with open(dataset) as f:\n",
    "        data=f.readlines()\n",
    "        attributes=data[0].rstrip().split(',')[:-1]\n",
    "        instances=[entry.rstrip().split(',')[:-1] for entry in data[1:]]\n",
    "        dataArray=[]\n",
    "        for i in range(len(instances[0])):\n",
    "            dataArray.append([float(instance[i]) for instance in instances])\n",
    "        instances=np.array(dataArray).T\n",
    "        labels=[entry.rstrip().split(',')[-1] for entry in data[1:]]\n",
    "        return attributes,instances,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L_bpkY00Gl6x"
   },
   "source": [
    "## Training: Building a Decision Tree Classifier ##\n",
    "\n",
    "The cell below asks for a dataset. It trains a decision tree classifier. \n",
    "\n",
    "We provide two classification datasets that could be applied to the decision tree algorithms. \n",
    "* \"iris.csv\" has four attributes with continuous values describing three different iris species.\n",
    "* \"lenses.csv\" contains four attributes with discrete values and three classes.\n",
    "\n",
    "Before training your classifier, run the cell below to take a look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QaGF8KJRGhQY"
   },
   "outputs": [],
   "source": [
    "dataset=input('Please Enter Your Dataset:')\n",
    "df=pd.read_csv(dataset)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hOcX98MkGsGw"
   },
   "source": [
    "The following cell gives the number of instances in all distinct classes.\n",
    "\n",
    "Label Name is the name of the last column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3xOFuckgGsfX"
   },
   "outputs": [],
   "source": [
    "label_name = input('Enter The Label Name:')\n",
    "print(df[label_name].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "95RPiR-CG21a"
   },
   "source": [
    "Before we run the following cell, let's learn an important concept called feature encoding. Many classifiers only take numerical data and some datasets have features that are not numerical. For example, a feature can be the state that a person lives in. Those are called [categorical features](https://en.wikipedia.org/wiki/Categorical_variable). In that case,we need to encode categorical features into discrete values. This process is called feature encoding\n",
    "\n",
    "In our notebook, if your dataset contains categorical features, you will see the code rules in the cell below. In the next section, when you are prompted to input test set for prediction, the algorithm will automatically encode the relevant categorical features according to the code rules showned below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cCsuSUodG3Lh"
   },
   "outputs": [],
   "source": [
    "attributes,instances,labels=loadDataSet(dataset)\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(instances,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kN9x8TJUG9HI"
   },
   "source": [
    "## Visualizing a Decision Tree##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6flkKTxaHCGk"
   },
   "source": [
    "The following cell will generate a visualization of the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eOHcqZCuG_fP"
   },
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None,max_depth=5,\\\n",
    "feature_names=attributes,class_names=clf.classes_,label='all',\\\n",
    "filled=True,special_characters=True) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TU67ZUzHHKi6"
   },
   "source": [
    "## Evaluating the Accuracy of a Decision Tree Classifier##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dFts6pt-HTVX"
   },
   "source": [
    "The following cell will run cross-validation to evaluate your decision tree classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I7chxB_7HWRq"
   },
   "source": [
    "K Fold Cross-Validation is used to estimate prediction error. The dataset is randomly divided into K folds. The first fold acts as the validation set while the method is fit on remaining K-1 folds. Mean Squared Error is calculated on the observations from the held-out fold. The process is repeated K times, taking a different part each time. \n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=12dKPXQ3m_-283DfvC2Tgy0rOU5gnY277)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bJwt-yp6HPf5"
   },
   "outputs": [],
   "source": [
    "n_foldCV=int(input(\"Please Enter the Number of Folds:\"))\n",
    "attributes,instances,labels=loadDataSet(dataset)\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(instances,labels)\n",
    "scores = cross_val_score(clf, instances, labels, cv=n_foldCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H5gXsFQcHbz8"
   },
   "source": [
    "You can visualize the testing and trainging in KFold cross-validation in the following cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x3ElIQJXHfDR"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(dataset)\n",
    "fig, ax = plt.subplots()\n",
    "cv = KFold(n_foldCV)\n",
    "if label_name == \"type of iris plant\":\n",
    "  df[label_name]=df[label_name].map({'Iris-setosa':'0','Iris-versicolor':'1', 'Iris-virginica':'2'})\n",
    "  X = np.array(df.iloc[:, 0:4])\n",
    "  y = np.array(df.iloc[:, 4:5])\n",
    "if label_name == \"type of lens\":\n",
    "  sorted_df = df.sort_values('type of lens')\n",
    "  X = np.array(sorted_df.iloc[:, 0:4])\n",
    "  y = np.array(sorted_df.iloc[:, 4:5])\n",
    "y = y.astype(np.int)\n",
    "plot_cv_indices(cv, X, y, ax, n_foldCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AIp1zl00HlKC"
   },
   "source": [
    "The following cell will output the accuracy score for each fold and the accuracy estimate of the model under 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S_DqDYgRHzB_"
   },
   "outputs": [],
   "source": [
    "print(\"Sores:\")\n",
    "[print(score) for score in scores]\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tm9KF6VFYyUf"
   },
   "source": [
    "## Prediction: Classifying New Instances Using a Decision Tree Classifier##\n",
    "\n",
    "The cell below classifies new instances with the decision tree you created.\n",
    "\n",
    "When you are prompted to input a prediction set, please create an example of an instance that looks like the instances in the training set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V2s3JmipYyvt"
   },
   "outputs": [],
   "source": [
    "testset=input('Please Enter Your Prediction Set:')\n",
    "testset=testset.strip().split(\",\")\n",
    "temp=[]\n",
    "for i in range(len(testset)):\n",
    "        temp.append(float(testset[i]))\n",
    "testset=np.array(temp).reshape((1,len(temp)))\n",
    "predictions=clf.predict(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JVGYr65DY4ha"
   },
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Notebook1_Decision_Tree_Classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
